# main.py
from datetime import datetime, timezone, timedelta
import arxiv
import os

# ==================== é…ç½®åŒº ====================
# ä» queries.txt è¯»å–æŸ¥è¯¢è¯­å¥
with open("queries.txt", "r", encoding="utf-8") as file:
    QUERIES = [line.strip() for line in file.readlines() if line.strip()]

# æ ¹ä¸‹è½½ç›®å½•
ROOT_DOWNLOAD_DIR = "./pdf"

# æ¯ä¸ªæŸ¥è¯¢æœ€å¤šä¸‹è½½å¤šå°‘ç¯‡è®ºæ–‡
MAX_RESULTS_PER_QUERY = 20  # ç²¾ç®€ï¼šåªä¸‹è½½20ç¯‡

# æŠ“å–æœ€è¿‘1å¤©çš„è®ºæ–‡
DAYS_AGO = 1
# ===============================================

# è®¡ç®—æ—¶é—´çª—å£ï¼šåªæŠ“ä»è¿™ä¸ªæ—¥æœŸä¹‹åæäº¤çš„è®ºæ–‡
CUTOFF_DATE = (datetime.now(timezone.utc).date() - timedelta(days=DAYS_AGO))

# åˆ›å»ºæ—¶é—´æˆ³ç›®å½•ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD_HH-MM-SSï¼‰
TIMESTAMP = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
SESSION_DIR = os.path.join(ROOT_DOWNLOAD_DIR, TIMESTAMP)
LOG_FILE = os.path.join(SESSION_DIR, "log.txt")

# åˆ›å»ºä¼šè¯ç›®å½•
os.makedirs(SESSION_DIR, exist_ok=True)

# å†™æ—¥å¿—å¤´
with open(LOG_FILE, "w", encoding="utf-8") as f:
    f.write(f"ğŸ“… æ–‡çŒ®æŠ“å–æ—¥å¿— - {TIMESTAMP}\n")
    f.write(f"ğŸ“¥ æ¯ç±»æœ€å¤šä¸‹è½½: {MAX_RESULTS_PER_QUERY} ç¯‡\n")
    f.write(f"{'='*50}\n")

def is_within_time_window(paper):
    """
    åˆ¤æ–­è®ºæ–‡æ˜¯å¦åœ¨è®¾å®šçš„æ—¶é—´çª—å£å†…ï¼ˆå³æœ€è¿‘ DAYS_AGO å¤©å†…ï¼‰
    """
    paper_date = paper.published.date()
    return paper_date >= CUTOFF_DATE

def sanitize_folder_name(query):
    """
    å°†æŸ¥è¯¢è¯­å¥æ˜ å°„ä¸ºç®€çŸ­çš„åˆ†ç±»æ–‡ä»¶å¤¹å
    """
    q = query.lower()
    if "graph" in q or "gnn" in q or "geometric" in q or "equivariant" in q or "diffusion" in q:
        return "gnn-diffusion"
    if "causal" in q or "invariant" in q or "disentanglement" in q:
        return "causal-representation"
    if "time" in q or "temporal" in q or "dynamics" in q or "kinetic" in q:
        return "time-series-dynamics"
    if "large language" in q or "llm" in q or "reinforcement" in q or "molecule generation" in q:
        return "llm-rl-generation"
    if "agent" in q or "continual" in q or "lifelong" in q or "autonomous" in q or "closed-loop" in q:
        return "agent-cl-autonomous"
    return "others"

def fetch_papers(query):
    """
    æŠ“å–å•ä¸ªæŸ¥è¯¢è¯­å¥ä¸‹çš„è®ºæ–‡
    """
    folder_name = sanitize_folder_name(query)
    category_dir = os.path.join(SESSION_DIR, folder_name)

    print(f"\nğŸ” æœç´¢: {query}")

    # æ„å»ºæœç´¢å¯¹è±¡
    search = arxiv.Search(
        query=query,
        max_results=MAX_RESULTS_PER_QUERY,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending,
    )

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = arxiv.Client(
        page_size=min(MAX_RESULTS_PER_QUERY, 100),
        delay_seconds=2,  # ç²¾ç®€ï¼šå‡å°‘å»¶è¿Ÿ
        num_retries=2     # ç²¾ç®€ï¼šå‡å°‘é‡è¯•æ¬¡æ•°
    )

    downloaded = 0
    try:
        for result in client.results(search):
            # æ—¶é—´è¿‡æ»¤ï¼šåªä¿ç•™æœ€è¿‘1å¤©çš„è®ºæ–‡
            if not is_within_time_window(result):
                continue

            # é˜²æ­¢è¶…é™
            if downloaded >= MAX_RESULTS_PER_QUERY:
                break

            # ç¬¬ä¸€æ¬¡ä¸‹è½½æ—¶åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹
            if downloaded == 0:
                os.makedirs(category_dir, exist_ok=True)

            # ä¸‹è½½è®¾ç½®
            short_id = result.get_short_id()
            filename = f"{short_id}.pdf"

            try:
                result.download_pdf(dirpath=category_dir, filename=filename)
                print(f"âœ… {short_id}: {result.title[:50]}...")

                # ç²¾ç®€æ—¥å¿—
                with open(LOG_FILE, "a", encoding="utf-8") as f:
                    f.write(f"{short_id} | {result.title[:60]} | {result.published.strftime('%m-%d %H:%M')}\n")

                downloaded += 1
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥ {short_id}: {e}")

    except Exception as e:
        print(f"âŒ æŸ¥è¯¢æ‰§è¡Œå‡ºé”™: {e}")

    print(f"âœ… æœ¬ç±»åˆ«å…±ä¸‹è½½ {downloaded} ç¯‡")
    return downloaded

# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    print(f"ğŸ“… å¼€å§‹æŠ“å–ã€æœ€è¿‘ {DAYS_AGO} å¤©ã€‘çš„æ–°è®ºæ–‡...")

    total_downloaded = 0
    for query in QUERIES:
        count = fetch_papers(query)
        total_downloaded += count

    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼å…±ä¸‹è½½ {total_downloaded} ç¯‡æ–°è®ºæ–‡ã€‚")
