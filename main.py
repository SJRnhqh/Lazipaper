# main.py
from datetime import datetime, timezone, timedelta
import arxiv
import os

# ==================== é…ç½®åŒº ====================
# ä» queries.txt è¯»å–æŸ¥è¯¢è¯­å¥
with open("queries.txt", "r", encoding="utf-8") as file:
    QUERIES = [line.strip() for line in file.readlines() if line.strip()]

# æ ¹ä¸‹è½½ç›®å½•
ROOT_DOWNLOAD_DIR = "./pdf"

# æ¯ä¸ªæŸ¥è¯¢æœ€å¤šä¸‹è½½å¤šå°‘ç¯‡è®ºæ–‡
MAX_RESULTS_PER_QUERY = 50  # å¯è‡ªç”±è°ƒæ•´ï¼š10ï¼ˆæ—¥å¸¸ï¼‰ã€50ï¼ˆå‘¨æŠ¥ï¼‰ã€100ï¼ˆè°ƒç ”ï¼‰

# æ˜¯å¦åªæŠ“å–æœ€è¿‘ N å¤©çš„è®ºæ–‡ï¼Ÿ
DAYS_AGO = 7           # â† è®¾ç½®ä¸º 7 è¡¨ç¤ºâ€œæœ€è¿‘ä¸€å‘¨â€
ONLY_TODAY = True      # â† è®¾ä¸º True è¡¨ç¤ºå¯ç”¨æ—¶é—´è¿‡æ»¤
# ===============================================

# è®¡ç®—æ—¶é—´çª—å£ï¼šåªæŠ“ä»è¿™ä¸ªæ—¥æœŸä¹‹åæäº¤çš„è®ºæ–‡
CUTOFF_DATE = (datetime.now(timezone.utc).date() - timedelta(days=DAYS_AGO))

# åˆ›å»ºæ—¶é—´æˆ³ç›®å½•ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD_HH-MM-SSï¼‰
TIMESTAMP = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
SESSION_DIR = os.path.join(ROOT_DOWNLOAD_DIR, TIMESTAMP)
LOG_FILE = os.path.join(SESSION_DIR, "log.txt")

# åˆ›å»ºä¼šè¯ç›®å½•
os.makedirs(SESSION_DIR, exist_ok=True)

# å†™æ—¥å¿—å¤´
with open(LOG_FILE, "w", encoding="utf-8") as f:
    f.write(f"ğŸ“… æ–‡çŒ®æŠ“å–æ—¥å¿—\n")
    f.write(f"â° æ‰§è¡Œæ—¶é—´: {TIMESTAMP}\n")
    f.write(f"ğŸ“… æŠ“å–èŒƒå›´: æœ€è¿‘ {DAYS_AGO} å¤©ï¼ˆä» {CUTOFF_DATE} å¼€å§‹ï¼‰\n")
    f.write(f"ğŸ” æŸ¥è¯¢ç±»åˆ«æ•°: {len(QUERIES)}\n")
    f.write(f"ğŸ“¥ æ¯ç±»æœ€å¤šä¸‹è½½: {MAX_RESULTS_PER_QUERY} ç¯‡\n")
    f.write(f"{'='*80}\n\n")

def is_within_time_window(paper):
    """
    åˆ¤æ–­è®ºæ–‡æ˜¯å¦åœ¨è®¾å®šçš„æ—¶é—´çª—å£å†…ï¼ˆå³æœ€è¿‘ DAYS_AGO å¤©å†…ï¼‰
    """
    paper_date = paper.published.date()
    return paper_date >= CUTOFF_DATE

def sanitize_folder_name(query):
    """
    å°†æŸ¥è¯¢è¯­å¥æ˜ å°„ä¸ºç®€çŸ­çš„åˆ†ç±»æ–‡ä»¶å¤¹å
    """
    q = query.lower()
    if "graph" in q or "gnn" in q or "geometric" in q or "equivariant" in q or "diffusion" in q:
        return "gnn-diffusion"
    if "causal" in q or "invariant" in q or "disentanglement" in q:
        return "causal-representation"
    if "time" in q or "temporal" in q or "dynamics" in q or "kinetic" in q:
        return "time-series-dynamics"
    if "large language" in q or "llm" in q or "reinforcement" in q or "molecule generation" in q:
        return "llm-rl-generation"
    if "agent" in q or "continual" in q or "lifelong" in q or "autonomous" in q or "closed-loop" in q:
        return "agent-cl-autonomous"
    return "others"

def fetch_papers(query):
    """
    æŠ“å–å•ä¸ªæŸ¥è¯¢è¯­å¥ä¸‹çš„è®ºæ–‡
    """
    folder_name = sanitize_folder_name(query)
    category_dir = os.path.join(SESSION_DIR, folder_name)

    print(f"\nğŸ” æœç´¢: {query}")
    print(f"ğŸ“ åˆ†ç±»æ–‡ä»¶å¤¹: {folder_name}")

    # æ„å»ºæœç´¢å¯¹è±¡
    search = arxiv.Search(
        query=query,
        max_results=MAX_RESULTS_PER_QUERY,           # æ§åˆ¶æ€»è¯·æ±‚æ•°
        sort_by=arxiv.SortCriterion.SubmittedDate,   # æŒ‰æäº¤æ—¶é—´æ’åº
        sort_order=arxiv.SortOrder.Descending,       # æœ€æ–°çš„åœ¨å‰
    )

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = arxiv.Client(
        page_size=min(MAX_RESULTS_PER_QUERY, 100),   # æ¯é¡µè¯·æ±‚æ•°
        delay_seconds=3,                             # è¯·æ±‚é—´éš”ï¼Œå°Šé‡æœåŠ¡å™¨
        num_retries=3                                # ç½‘ç»œå¤±è´¥æ—¶é‡è¯•æ¬¡æ•°
    )

    downloaded = 0
    try:
        for result in client.results(search):
            # æ—¶é—´è¿‡æ»¤ï¼šåªä¿ç•™æœ€è¿‘ N å¤©çš„è®ºæ–‡
            if ONLY_TODAY and not is_within_time_window(result):
                continue  # è·³è¿‡å¤ªæ—©çš„è®ºæ–‡

            # é˜²æ­¢è¶…é™
            if downloaded >= MAX_RESULTS_PER_QUERY:
                break

            # ç¬¬ä¸€æ¬¡ä¸‹è½½æ—¶åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹
            if downloaded == 0:
                os.makedirs(category_dir, exist_ok=True)

            # ä¸‹è½½è®¾ç½®
            short_id = result.get_short_id()
            filename = f"{short_id}.pdf"
            filepath = os.path.join(category_dir, filename)

            try:
                result.download_pdf(dirpath=category_dir, filename=filename)
                print(f"âœ… {short_id}: {result.title[:70]}...")

                # å†™å…¥æ—¥å¿—
                with open(LOG_FILE, "a", encoding="utf-8") as f:
                    f.write(f"ğŸ“„ è®ºæ–‡ID: {short_id}\n")
                    f.write(f"   æ ‡é¢˜: {result.title}\n")
                    f.write(f"   ä½œè€…: {', '.join(str(author) for author in result.authors)}\n")
                    f.write(f"   æäº¤æ—¶é—´: {result.published.strftime('%Y-%m-%d %H:%M')} (UTC)\n")
                    f.write(f"   é“¾æ¥: {result.entry_id}\n")
                    f.write(f"   ä¿å­˜è·¯å¾„: {filepath}\n")
                    f.write(f"{'-'*80}\n")

                downloaded += 1
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥ {short_id}: {e}")

    except Exception as e:
        print(f"âŒ æŸ¥è¯¢æ‰§è¡Œå‡ºé”™: {e}")

    print(f"âœ… æœ¬ç±»åˆ«å…±ä¸‹è½½ {downloaded} ç¯‡")
    return downloaded

# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    print(f"ğŸ“… å¼€å§‹æŠ“å–ã€æœ€è¿‘ {DAYS_AGO} å¤©ã€‘çš„æ–°è®ºæ–‡...")
    print(f"ğŸ“ æœ¬æ¬¡ç»“æœå°†ä¿å­˜åœ¨: ./{SESSION_DIR}/")

    total_downloaded = 0
    for query in QUERIES:
        count = fetch_papers(query)
        total_downloaded += count

    # æ›´æ–°æ—¥å¿—å¤´éƒ¨ç»Ÿè®¡
    with open(LOG_FILE, "r+", encoding="utf-8") as f:
        content = f.read()
        f.seek(0, 0)
        f.write(f"ğŸ“Š æ€»è®¡ä¸‹è½½: {total_downloaded} ç¯‡è®ºæ–‡\n")
        f.write(f"{'='*80}\n\n")
        f.write(content)

    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼å…±ä¸‹è½½ {total_downloaded} ç¯‡æ–°è®ºæ–‡ã€‚")
    print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜: {LOG_FILE}")